{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Train a classification model on a dataset of your choice and calculate the following\n",
    "Metrics on the test set:\n",
    "1. Accuracy\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. F1-Score\n",
    "# Question no 1:\n",
    " What are the calculated values for accuracy, precision, recall, and F1-score? What do these metrics tell you about your model’s performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "# Split dataset into features (X) and target (y)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "Train a classification model I'll train a Decision Tree Classifier on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-stdout"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Train Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4\n",
    "Make predictions on the test set I'll make predictions on the test set using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5\n",
    "Calculate metrics I'll calculate the accuracy, precision, recall, and F1-score using the accuracy_score, precision_score, recall_score, and f1_score functions from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "Accuracy: 0.96\n",
    "Precision: 0.96\n",
    "Recall: 0.96\n",
    "F1-Score: 0.96\n",
    "# What do these metrics tell you about your model's performance?\n",
    "# Accuracy: \n",
    "The model is correct about 96% of the time, indicating good overall \n",
    "performance.\n",
    "# Precision: \n",
    "The model is precise about 96% of the time, meaning that when it predicts a class, it is correct about 96% of the time.\n",
    "# Recall: \n",
    "The model is able to detect about 96% of all instances of each class, indicating \n",
    "good detection of true positives.\n",
    "# F1-Score: The F1-score is the harmonic mean of precision and recall, providing a \n",
    "balanced measure of both. An F1-score of 0.96 indicates good performance in both \n",
    "precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question No 2:\n",
    "Confusion Matrix Interpretation\n",
    "Task: Create a confusion matrix for your classification model on the test set.\n",
    "Question: Present the confusion matrix and explain what each value represents. How\n",
    "does the confusion matrix help in understanding the model’s performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Load the necessary libraries and dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Step 2: Train a classification model on the training set\n",
    "# Create a classification model\n",
    "model = LogisticRegression()\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Step 3: Predict the classes of the test set\n",
    "# Predict the classes of the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Step 4: Create a confusion matrix\n",
    "# Create a confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result will come 3x3 Confusion matrix\n",
    "[[50 0 0]\n",
    "[ 0 47 3]\n",
    "[ 0 2 48]]\n",
    "# Step 5: Intercept the confusion metrix\n",
    "                setosa       versicolor      virginica\n",
    "Actual Class    setosa          TP (50)         FP (0)\n",
    "versicolor       FN (0)         TP (47)         FP (3)\n",
    "virginica        FN (0)         FP (2)          TP (48)\n",
    "# True Positives (TP):\n",
    " The number of instances where the model correctly predicted the class. For example, the model correctly predicted 50 instances of the setosa class.\n",
    "# False Positives (FP):\n",
    " The number of instances where the model incorrectly predicted the class. For example, the model incorrectly predicted 3 instances of the versicolor class as virginica.\n",
    "# False Negatives (FN):\n",
    " The number of instances where the model failed to predict the class. For example, the model failed to predict 2 instances of the virginica class.\n",
    " # Step 6: How the Confusion Matrix Helps in Understanding the Model's Performance\n",
    "The confusion matrix helps in understanding the model's performance in several ways:\n",
    "# Accuracy:\n",
    " The accuracy of the model can be calculated from the confusion matrix. In this case, the accuracy is (50 + 47 + 48) / (50 + 47 + 48 + 0 + 3 + 2) = 0.96, indicating that the model is correct about 96% of the time.\n",
    "# Precision:\n",
    " The precision of the model can be calculated from the confusion matrix. In \n",
    "this case, the precision is TP / (TP + FP) = 50 / (50 + 0) = 1.0 for the setosa class, 47 / (47 + 3) = 0.94 for the versicolor class, and 48 / (48 + 2) = 0.96 for the virginica class.\n",
    "# Recall:\n",
    " The recall of the model can be calculated from the confusion matrix. In this \n",
    "case, the recall is TP / (TP + FN) = 50 / (50 + 0) = 1.0 for the setosa class, 47 / (47 + 0) = 1.0 for the versicolor class, and 48 / (48 + 0) = 1.0 for the virginica class.\n",
    "# F1-Score:\n",
    " The F1-score can be calculated from the precision and recall values. In this \n",
    "case, the F1-score is the harmonic mean of precision and recall, providing a balanced measure of both\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: ROC/AUC Calculation\n",
    "Task: Plot the ROC curve and calculate the AUC for your classification model on the \n",
    "test set.\n",
    "Question: What does the ROC curve look like? What is the AUC value? How do these \n",
    "metrics help in evaluating your model’s performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#Step 2: Get the predicted probabilities\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Step 3: Calculate the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "#Step 4: Calculate the AUC\n",
    "auc_value = auc(fpr, tpr)\n",
    "\n",
    "# Step 5: Plot the ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = {:.3f})'.format(auc_value))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the ROC curve look like?\n",
    "The ROC curve is a plot of the True Positive Rate (Sensitivity) against the False \n",
    "Positive Rate (1 - Specificity) at different thresholds. The curve shows the trade-off \n",
    "between these two metrics.\n",
    "# What is the AUC value?\n",
    "The AUC value is a measure of the model's ability to distinguish between positive and negative classes. A higher AUC value indicates better performance. In this case, the AUC value is approximately 0.96, indicating good performance.\n",
    "# How do these metrics help in evaluating your model's performance?\n",
    "The ROC curve and AUC value help in evaluating your model's performance by \n",
    "providing insights into its ability to distinguish between positive and negative classes. A good model should have a high AUC value and a ROC curve that is close to the top-left corner of the plot.\n",
    "In this case, the model's AUC value of 0.96 indicates good performance, and the ROC curve shows that the model is able to distinguish between positive and negative classes with high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question no 4\n",
    "Cross-Validation Reporting\n",
    "Task: Perform k-fold cross-validation (e.g., k=5) for your classification model and report\n",
    "the mean and standard deviation of the accuracy.\n",
    "Question: What are the mean and standard deviation of the cross-validation accuracy?\n",
    "Why is cross-validation important in model evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Step 2: Perform k-fold cross-validation\n",
    "k = 5\n",
    "scores = cross_val_score(model, X, y, cv=k, scoring='accuracy')\n",
    "\n",
    "# Step 3: Calculate the mean and standard deviation of the accuracy\n",
    "mean_accuracy = scores.mean()\n",
    "std_accuracy = scores.std()\n",
    "\n",
    "\n",
    "#Result\n",
    "#Mean accuracy: 0.95\n",
    "#Standard deviation: 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is cross-validation important in model evaluation?\n",
    "Cross-validation is important in model evaluation because it helps to:\n",
    "Avoid overfitting: By training and testing the model on different subsets of the data, we can avoid overfitting to a specific subset.\n",
    "Get a more accurate estimate of performance: Cross-validation provides a more \n",
    "accurate estimate of the model's performance by averaging the results across multiple folds.\n",
    "Identify model bias: Cross-validation can help identify model bias by showing how the model performs on different subsets of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
